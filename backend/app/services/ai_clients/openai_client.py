"""OpenAI å®¢æˆ·ç«¯ - ä½¿ç”¨å®˜æ–¹ OpenAI Python SDK"""
import json
from typing import Any, AsyncGenerator, Dict, Optional, Union

from openai import AsyncOpenAI, AsyncStream
from openai.types.chat import ChatCompletion, ChatCompletionChunk
from openai.types.chat.chat_completion_message import ChatCompletionMessage
from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall

from app.logger import get_logger
from .base_client import BaseAIClient

logger = get_logger(__name__)


class OpenAIClient(BaseAIClient):
    """OpenAI API å®¢æˆ·ç«¯ - ä½¿ç”¨å®˜æ–¹ OpenAI Python SDK"""

    def __init__(self, api_key: str, base_url: str, config=None):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        super().__init__(api_key, base_url, config)
        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url.rstrip("/") + "/" if not base_url.endswith("/") else base_url
        )
        logger.info(f"âœ… åˆ›å»º OpenAI SDK å®¢æˆ·ç«¯: base_url={base_url}")

    def _build_headers(self) -> Dict[str, str]:
        """æ„å»ºè¯·æ±‚å¤´ - ä½¿ç”¨ OpenAI SDK ä¸éœ€è¦æ‰‹åŠ¨æ„å»º"""
        return {}

    def _build_payload(
        self,
        messages: list,
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[list] = None,
        tool_choice: Optional[str] = None,
        stream: bool = False,
    ) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚è½½è· - ç”¨äºæ—¥å¿—è®°å½•"""
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream,
        }
        if tools:
            payload["tools"] = tools
            if tool_choice:
                payload["tool_choice"] = tool_choice
        
        # è®°å½•æµå¼è¯·æ±‚çš„payloadï¼ˆè°ƒè¯•ç”¨ï¼‰
        if stream:
            logger.info(f"ğŸ“¤ OpenAI æµå¼è¯·æ±‚ payload (model={model}, stream={stream}): {json.dumps(payload, ensure_ascii=False)[:500]}")
        
        return payload

    async def chat_completion(
        self,
        messages: list,
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[list] = None,
        tool_choice: Optional[str] = None,
    ) -> Dict[str, Any]:
        """èŠå¤©è¡¥å…¨ - ä½¿ç”¨ OpenAI SDK"""
        # DeepSeek æ¨¡å‹é™åˆ¶ max_tokens ä¸º 8192
        if "deepseek" in model.lower() and max_tokens > 8192:
            logger.warning(f"âš ï¸  DeepSeek æ¨¡å‹ max_tokens é™åˆ¶ä¸º 8192ï¼Œå°† {max_tokens} è°ƒæ•´ä¸º 8192")
            max_tokens = 8192
        
        # è®°å½•è¯·æ±‚
        logger.info(f"ğŸ“¤ OpenAI è¯·æ±‚: model={model}, messages={len(messages)}")
        
        try:
            # è°ƒç”¨ OpenAI SDK
            response: ChatCompletion = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                tools=tools,
                tool_choice=tool_choice,
                stream=False
            )
            
            # è®°å½•å“åº”
            logger.debug(f"ğŸ“¥ OpenAI åŸå§‹å“åº”: {response}")
            
            choice = response.choices[0]
            message: ChatCompletionMessage = choice.message
            
            # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_calls = None
            if message.tool_calls:
                tool_calls = []
                for tc in message.tool_calls:
                    tool_calls.append({
                        "id": tc.id,
                        "type": tc.type,
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    })
            
            return {
                "content": message.content or "",
                "tool_calls": tool_calls,
                "finish_reason": choice.finish_reason,
            }
            
        except Exception as e:
            logger.error(f"âŒ OpenAI è¯·æ±‚å¤±è´¥: {type(e).__name__}: {str(e)}")
            raise

    async def chat_completion_stream(
        self,
        messages: list,
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[list] = None,
        tool_choice: Optional[str] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼ç”Ÿæˆï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ - ä½¿ç”¨ OpenAI SDK
        
        Yields:
            Dict with keys:
            - content: str - æ–‡æœ¬å†…å®¹å—
            - tool_calls: list - å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
            - done: bool - æ˜¯å¦ç»“æŸ
        """
        # DeepSeek æ¨¡å‹é™åˆ¶ max_tokens ä¸º 8192
        if "deepseek" in model.lower() and max_tokens > 8192:
            logger.warning(f"âš ï¸  DeepSeek æ¨¡å‹ max_tokens é™åˆ¶ä¸º 8192ï¼Œå°† {max_tokens} è°ƒæ•´ä¸º 8192")
            max_tokens = 8192
        
        logger.info(f"ğŸ“¤ OpenAI æµå¼è¯·æ±‚: model={model}, base_url={self.base_url}, tools={len(tools) if tools else 0}")
        
        try:
            # è°ƒç”¨ OpenAI SDK æµå¼ API
            stream: AsyncStream[ChatCompletionChunk] = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                tools=tools,
                tool_choice=tool_choice,
                stream=True
            )
            
            tool_calls_buffer = {}  # æ”¶é›†å·¥å…·è°ƒç”¨å—
            
            async for chunk in stream:
                if not chunk.choices:
                    continue
                    
                choice = chunk.choices[0]
                delta = choice.delta
                
                # æ£€æŸ¥å·¥å…·è°ƒç”¨
                tc_list = delta.tool_calls
                if tc_list:
                    for tc in tc_list:
                        index = tc.index
                        if index not in tool_calls_buffer:
                            tool_calls_buffer[index] = {
                                "id": tc.id or "",
                                "type": tc.type,
                                "function": {
                                    "name": tc.function.name if tc.function else "",
                                    "arguments": tc.function.arguments if tc.function else ""
                                }
                            }
                        else:
                            existing = tool_calls_buffer[index]
                            # åˆå¹¶ function.arguments
                            if tc.function and tc.function.arguments:
                                existing["function"]["arguments"] = (
                                    existing["function"].get("arguments", "") +
                                    tc.function.arguments
                                )
                
                # æ£€æŸ¥æ–‡æœ¬å†…å®¹
                content = delta.content
                if content:
                    yield {"content": content}
            
            # æµç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨éœ€è¦å¤„ç†
            if tool_calls_buffer:
                yield {"tool_calls": list(tool_calls_buffer.values()), "done": True}
            yield {"done": True}
            
        except Exception as e:
            logger.error(f"âŒ OpenAI æµå¼è¯·æ±‚å¤±è´¥: {type(e).__name__}: {str(e)}")
            raise